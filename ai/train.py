import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import os
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler # å¯¼å…¥æ··åˆç²¾åº¦å·¥å…·

from model import CatInvTransformer
from data_loader import CADRefineDataset

# --- å¼ºåˆ¶æ£€æŸ¥ GPU ---
if not torch.cuda.is_available():
    print("âŒ é”™è¯¯: æœªæ£€æµ‹åˆ° GPUï¼è¯·æ£€æŸ¥ PyTorch æ˜¯å¦å®‰è£…äº† CUDA ç‰ˆæœ¬ã€‚")
    print("æç¤º: pip install torch --index-url https://download.pytorch.org/whl/cu121")
    exit()

DEVICE = torch.device("cuda")
print(f"âœ… æˆåŠŸè¯†åˆ«æ˜¾å¡: {torch.cuda.get_device_name(0)}")

# --- é…ç½®å‚æ•° ---
BATCH_SIZE = 128   # æœ‰äº† 4060 å’Œ AMPï¼ŒBatch å¯ä»¥è°ƒå¤§ä¸€ç‚¹
LR = 1e-4
EPOCHS = 100
MAX_SEQ_LEN = 100
INPUT_DIM = 33
SAVE_DIR = "checkpoints"

if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

def train():
    # 1. å‡†å¤‡æ•°æ®
    train_ds = CADRefineDataset("train.txt", max_len=MAX_SEQ_LEN)
    val_ds = CADRefineDataset("val.txt", max_len=MAX_SEQ_LEN)
    
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)

    # 2. åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = CatInvTransformer(input_dim=INPUT_DIM, max_seq_len=MAX_SEQ_LEN).to(DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-2) # ä½¿ç”¨ AdamW
    
    # 3. æŸå¤±å‡½æ•°å’Œæ··åˆç²¾åº¦ç¼©æ”¾å™¨
    criterion = nn.MSELoss(reduction='none')
    scaler = GradScaler() # è‡ªåŠ¨ç¼©æ”¾æ¢¯åº¦ï¼Œé˜²æ­¢ FP16 æº¢å‡º

    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        for batch in pbar:
            inputs = batch["input"].to(DEVICE)
            targets = batch["target"].to(DEVICE)
            masks = batch["mask"].to(DEVICE)
            
            optimizer.zero_grad()
            
            # --- æ ¸å¿ƒä¼˜åŒ–ï¼šä½¿ç”¨ autocast è¿›è¡Œæ··åˆç²¾åº¦å‰å‘ä¼ æ’­ ---
            with autocast():
                outputs = model(inputs, src_key_padding_mask=masks)
                
                # è®¡ç®—æœ‰æ•ˆä½çš„ Loss
                active_mask = (~masks).float() # (B, S)
                loss_matrix = criterion(outputs, targets).mean(dim=-1) # (B, S)
                loss = (loss_matrix * active_mask).sum() / active_mask.sum()
            
            # --- æ ¸å¿ƒä¼˜åŒ–ï¼šæ¢¯åº¦ç¼©æ”¾å’Œåå‘ä¼ æ’­ ---
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            train_loss += loss.item()
            pbar.set_postfix({"loss": f"{loss.item():.6f}"})

        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in val_loader:
                inputs = batch["input"].to(DEVICE)
                targets = batch["target"].to(DEVICE)
                masks = batch["mask"].to(DEVICE)
                
                with autocast():
                    outputs = model(inputs, src_key_padding_mask=masks)
                    active_mask = (~masks).float()
                    loss_matrix = criterion(outputs, targets).mean(dim=-1)
                    loss = (loss_matrix * active_mask).sum() / active_mask.sum()
                
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        print(f"âœ¨ Epoch {epoch+1} å®Œæˆ! Train Loss: {train_loss/len(train_loader):.6f}, Val Loss: {avg_val_loss:.6f}")
        
        # 4. è‡ªåŠ¨ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if (epoch + 1) % 10 == 0:
            save_path = f"{SAVE_DIR}/catinv_model_e{epoch+1}.pth"
            torch.save(model.state_dict(), save_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³ {save_path}")

if __name__ == "__main__":
    train()